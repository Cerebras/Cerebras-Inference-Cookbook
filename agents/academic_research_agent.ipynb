{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Research Agent\n",
    "\n",
    "**A conversational AI research assistant powered by PydanticAI + Cerebras + Unstructured.io**\n",
    "\n",
    "This cookbook demonstrates how to build a conversational agent that:\n",
    "- Generates diverse arXiv search queries\n",
    "- Searches and analyzes academic papers\n",
    "- Downloads and processes PDFs with Unstructured.io\n",
    "- Performs deep analysis and synthesizes research insights and saves them as reports\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **PydanticAI Agent Architecture** - Building conversational agents with tools\n",
    "2. **Cerebras Integration** - Using Cerebras LLMs with PydanticAI\n",
    "3. **Pydantic Schemas** - Type-safe structured outputs from LLMs\n",
    "4. **Unstructured.io** - High-quality PDF text extraction\n",
    "5. **Tool Design** - Creating effective agent tools with RunContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pydantic-ai cerebras-cloud-sdk python-dotenv requests feedparser unstructured-client pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API Keys\n",
    "\n",
    "Get API keys to get started with super fast inference, and Unstructured's powerful document procesing:\n",
    "- **Cerebras**: https://cloud.cerebras.ai (free tier available)\n",
    "- **Unstructured.io**: https://unstructured.io (free tier available)\n",
    "\n",
    "\n",
    "Next, we suggest to add the secrets in the Google Collab Password service, or via a .env file, if you cloned the repository.\n",
    "\n",
    "```bash\n",
    "CEREBRAS_API_KEY=your-key-here\n",
    "UNSTRUCTURED_API_KEY=your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "required = [\"CEREBRAS_API_KEY\", \"UNSTRUCTURED_API_KEY\"]\n",
    "missing = [k for k in required if not os.getenv(k)]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        f\"Missing API keys: {', '.join(missing)}. Add them to .env file.\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ API keys loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pydantic Schemas\n",
    "\n",
    "We are using Pydantic models for type safety. Pydantic is a production grade typing framework, that helps to create reliable LLM responses.\n",
    "These schemas:\n",
    "- Guide the LLM on expected output structure\n",
    "- Validate responses automatically\n",
    "- Provide type hints throughout the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ArxivQueries(BaseModel):\n",
    "    \"\"\"Structured output for arXiv query generation\"\"\"\n",
    "    queries: List[str] = Field(description=\"List of diverse search queries\")\n",
    "    reasoning: str = Field(description=\"Why these queries were chosen\")\n",
    "\n",
    "\n",
    "class AbstractAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of paper abstracts\"\"\"\n",
    "    key_themes: List[str] = Field(description=\"Main themes across papers\")\n",
    "    top_papers_for_deep_analysis: List[str] = Field(\n",
    "        description=\"arXiv IDs of most relevant papers\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Why these papers were selected\")\n",
    "\n",
    "\n",
    "class PaperAnalysis(BaseModel):\n",
    "    \"\"\"Deep analysis of a single paper\"\"\"\n",
    "    arxiv_id: str\n",
    "    methods: str = Field(description=\"Methods and architectures used\")\n",
    "    contributions: str = Field(description=\"Novel contributions\")\n",
    "    limitations: Optional[str] = Field(default=None)\n",
    "\n",
    "\n",
    "class ResearchDirection(BaseModel):\n",
    "    \"\"\"A future research direction\"\"\"\n",
    "    direction: str = Field(description=\"The research direction\")\n",
    "    rationale: str = Field(description=\"Why this is important\")\n",
    "\n",
    "\n",
    "class ResearchOutput(BaseModel):\n",
    "    \"\"\"Final comprehensive research output\"\"\"\n",
    "    research_landscape_summary: str = Field(\n",
    "        description=\"Overview of the research landscape\"\n",
    "    )\n",
    "    key_innovations: List[str] = Field(\n",
    "        description=\"Major innovations identified\"\n",
    "    )\n",
    "    future_research_directions: List[ResearchDirection] = Field(\n",
    "        description=\"Suggested future research directions\"\n",
    "    )\n",
    "    papers_analyzed: int = Field(description=\"Total papers analyzed\")\n",
    "    queries_used: List[str] = Field(description=\"Search queries used\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Schemas for Type Safety\n",
    "\n",
    "Here's how schemas validate LLM outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['vision language models', 'multimodal reasoning']\n",
      "Reasoning: These queries cover both architecture and capability aspects\n",
      "\n",
      "Papers analyzed: 10\n",
      "Innovations: ['Cross-modal attention', 'Chain-of-thought prompting']\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a validated ArxivQueries object\n",
    "example_queries: ArxivQueries = ArxivQueries(\n",
    "    queries=[\"vision language models\", \"multimodal reasoning\"],\n",
    "    reasoning=\"These queries cover both architecture and capability aspects\"\n",
    ")\n",
    "\n",
    "print(f\"Queries: {example_queries.queries}\")\n",
    "print(f\"Reasoning: {example_queries.reasoning}\")\n",
    "\n",
    "# Example: Creating a validated ResearchOutput\n",
    "example_output: ResearchOutput = ResearchOutput(\n",
    "    research_landscape_summary=\"The field is rapidly evolving...\",\n",
    "    key_innovations=[\"Cross-modal attention\", \"Chain-of-thought prompting\"],\n",
    "    future_research_directions=[\n",
    "        ResearchDirection(direction=\"Video reasoning\", rationale=\"Temporal understanding is key\")\n",
    "    ],\n",
    "    papers_analyzed=10,\n",
    "    queries_used=[\"vision language models\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nPapers analyzed: {example_output.papers_analyzed}\")\n",
    "print(f\"Innovations: {example_output.key_innovations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dependencies & Configuration\n",
    "\n",
    "The agent uses **dependency injection** via PydanticAI's `RunContext`. This allows tools to access shared resources like API clients and caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebras.cloud.sdk import AsyncCerebras\n",
    "from unstructured_client import UnstructuredClient\n",
    "\n",
    "\n",
    "class ResearchDeps(BaseModel):\n",
    "    \"\"\"Dependencies for the research agent\"\"\"\n",
    "    cerebras_client: Any\n",
    "    unstructured_client: Any\n",
    "    papers_cache: Dict[str, Dict[str, Any]] = Field(default_factory=dict)\n",
    "    start_year: int = 2020\n",
    "    max_papers_per_query: int = 15\n",
    "    max_papers_for_deep_analysis: int = 3\n",
    "    fulltext_excerpt_chars: int = 12000\n",
    "\n",
    "    model_config = {\"arbitrary_types_allowed\": True}\n",
    "\n",
    "\n",
    "def create_research_deps(\n",
    "    start_year: int = 2020,\n",
    "    max_papers_for_deep_analysis: int = 3\n",
    ") -> ResearchDeps:\n",
    "    \"\"\"Create research dependencies with API clients\"\"\"\n",
    "    return ResearchDeps(\n",
    "        cerebras_client=AsyncCerebras(\n",
    "            api_key=os.getenv(\"CEREBRAS_API_KEY\")\n",
    "        ),\n",
    "        unstructured_client=UnstructuredClient(\n",
    "            api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    "        ),\n",
    "        start_year=start_year,\n",
    "        max_papers_for_deep_analysis=max_papers_for_deep_analysis\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cerebras in Strict Mode\n",
    "\n",
    "**Important**: Cerebras requires all tools to have the same `strict` parameter value. PydanticAI may generate tools with mixed values, which causes errors. We proactively avoid this with a `prepare_tools` hook that normalizes all tools to `strict=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "from pydantic_ai.tools import ToolDefinition\n",
    "\n",
    "\n",
    "async def set_consistent_strict_param(\n",
    "    ctx: Any,\n",
    "    tool_defs: List[ToolDefinition]\n",
    ") -> List[ToolDefinition]:\n",
    "    \"\"\"\n",
    "    Enforce consistent strict=False for all tools.\n",
    "    \n",
    "    This addresses the error:\n",
    "    \"Tools with mixed values for 'strict' are not allowed\"\n",
    "    \"\"\"\n",
    "    return [replace(tool_def, strict=False) for tool_def in tool_defs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create the Agent\n",
    "\n",
    "Now we instantiate the PydanticAI agent with:\n",
    "- Cerebras `gpt-oss-120b` model\n",
    "- `ResearchDeps` for dependency injection\n",
    "- `prepare_tools` hook for strict mode\n",
    "- System prompt defining the agent's role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "agent = Agent(\n",
    "    'cerebras:gpt-oss-120b',\n",
    "    deps_type=ResearchDeps,\n",
    "    prepare_tools=set_consistent_strict_param,\n",
    "    system_prompt=\"\"\"You are an expert academic research assistant specializing in literature reviews.\n",
    "\n",
    "You help researchers by:\n",
    "1. Generating effective arXiv search queries\n",
    "2. Searching and analyzing academic papers\n",
    "3. Identifying key themes and innovations\n",
    "4. Suggesting future research directions\n",
    "\n",
    "You have access to tools for each step of the research process. Use them strategically\n",
    "to conduct comprehensive literature reviews. When asked to research a topic:\n",
    "\n",
    "1. First generate diverse search queries\n",
    "2. Search arXiv with those queries\n",
    "3. Analyze abstracts to identify most relevant papers\n",
    "4. Download and analyze full papers\n",
    "5. Synthesize findings into a comprehensive report\n",
    "\n",
    "Be thorough, cite specific papers, and provide actionable insights.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Define the 7 Research Tools\n",
    "\n",
    "In PydanticAI, each tool is decorated with `@agent.tool` and receives `RunContext[ResearchDeps]` for dependency access.\n",
    "\n",
    "### Tool 1: Generate arXiv Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "@agent.tool\n",
    "async def generate_arxiv_queries(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    topic: str,\n",
    "    num_queries: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate diverse arXiv search queries for a research topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic to generate queries for\n",
    "        num_queries: Number of queries to generate (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with queries and reasoning\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Generating {num_queries} search queries for: {topic}\")\n",
    "    \n",
    "    prompt = f\"\"\"Generate {num_queries} diverse arXiv search queries for researching: \"{topic}\"\n",
    "\n",
    "Make queries:\n",
    "- Specific and targeted\n",
    "- Cover different aspects/angles\n",
    "- Use relevant technical terms\n",
    "- Suitable for arXiv API search\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"queries\": [\"query1\", \"query2\", ...],\n",
    "  \"reasoning\": \"why these queries cover the topic well\"\n",
    "}}\"\"\"\n",
    "\n",
    "    response = await ctx.deps.cerebras_client.chat.completions.create(\n",
    "        model=\"gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=1.0,\n",
    "        max_completion_tokens=12000\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    data = json.loads(content)\n",
    "    \n",
    "    # Validate with Pydantic schema\n",
    "    result: ArxivQueries = ArxivQueries(**data)\n",
    "    \n",
    "    print(f\"‚úì Generated {len(result.queries)} queries\")\n",
    "    for i, q in enumerate(result.queries, 1):\n",
    "        print(f\"  {i}. {q}\")\n",
    "    \n",
    "    return json.dumps(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: Search arXiv Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "\n",
    "@agent.tool\n",
    "async def search_arxiv_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    queries: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Search arXiv with multiple queries and cache results.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of search query strings\n",
    "    \n",
    "    Returns:\n",
    "        Summary of papers found\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìö Searching arXiv with {len(queries)} queries...\")\n",
    "    \n",
    "    all_papers: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        search_url = \"http://export.arxiv.org/api/query\"\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": ctx.deps.max_papers_per_query,\n",
    "            \"sortBy\": \"relevance\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, params=params, timeout=30)\n",
    "            feed = feedparser.parse(response.content)\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                # Skip entries without required fields\n",
    "                if not hasattr(entry, 'id') or not hasattr(entry, 'published'):\n",
    "                    continue\n",
    "                if not hasattr(entry, 'title') or not hasattr(entry, 'summary'):\n",
    "                    continue\n",
    "                    \n",
    "                arxiv_id = entry.id.split(\"/abs/\")[-1]\n",
    "                \n",
    "                if arxiv_id not in all_papers:\n",
    "                    try:\n",
    "                        year = int(entry.published[:4])\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                    \n",
    "                    if year >= ctx.deps.start_year:\n",
    "                        authors = []\n",
    "                        if hasattr(entry, 'authors'):\n",
    "                            authors = [author.name for author in entry.authors if hasattr(author, 'name')]\n",
    "                        \n",
    "                        all_papers[arxiv_id] = {\n",
    "                            \"arxiv_id\": arxiv_id,\n",
    "                            \"title\": entry.title,\n",
    "                            \"authors\": authors,\n",
    "                            \"year\": year,\n",
    "                            \"abstract\": entry.summary,\n",
    "                            \"link\": getattr(entry, 'link', f\"https://arxiv.org/abs/{arxiv_id}\")\n",
    "                        }\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Query failed: {query[:50]}... ({str(e)[:50]})\")\n",
    "            continue\n",
    "        \n",
    "        await asyncio.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Cache papers in dependencies\n",
    "    ctx.deps.papers_cache.update(all_papers)\n",
    "    \n",
    "    summary = f\"Found {len(all_papers)} unique papers from {ctx.deps.start_year} onwards\\n\\n\"\n",
    "    summary += \"Top papers:\\n\"\n",
    "    for i, (arxiv_id, paper) in enumerate(list(all_papers.items())[:10], 1):\n",
    "        summary += f\"{i}. [{paper['year']}] {arxiv_id} ‚Äî {paper['title'][:80]}...\\n\"\n",
    "    \n",
    "    print(f\"‚úì Found {len(all_papers)} papers\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Analyze Paper Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "async def analyze_paper_abstracts(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    topic: str,\n",
    "    max_papers: int = 20\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Analyze paper abstracts to identify key themes and select papers for deep analysis.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic\n",
    "        max_papers: Maximum papers to analyze (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Analyzing abstracts for: {topic}\")\n",
    "    \n",
    "    papers = list(ctx.deps.papers_cache.values())[:max_papers]\n",
    "    \n",
    "    if not papers:\n",
    "        return json.dumps({\n",
    "            \"key_themes\": [],\n",
    "            \"top_papers_for_deep_analysis\": [],\n",
    "            \"reasoning\": \"No papers in cache. Run search_arxiv_papers first.\"\n",
    "        })\n",
    "    \n",
    "    abstracts_text = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Paper {i+1} (arXiv:{p['arxiv_id']})\\nTitle: {p['title']}\\nAbstract: {p['abstract']}\"\n",
    "        for i, p in enumerate(papers)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(papers)} paper abstracts for research on: \"{topic}\"\n",
    "\n",
    "{abstracts_text}\n",
    "\n",
    "Identify:\n",
    "1. Key themes across papers\n",
    "2. Top {ctx.deps.max_papers_for_deep_analysis} most relevant papers for deep analysis (by arXiv ID)\n",
    "3. Reasoning for selections\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"key_themes\": [\"theme1\", \"theme2\", ...],\n",
    "  \"top_papers_for_deep_analysis\": [\"arxiv_id1\", \"arxiv_id2\", ...],\n",
    "  \"reasoning\": \"explanation\"\n",
    "}}\"\"\"\n",
    "\n",
    "    response = await ctx.deps.cerebras_client.chat.completions.create(\n",
    "        model=\"gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=1.0,\n",
    "        max_completion_tokens=12000\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    data = json.loads(content)\n",
    "    \n",
    "    # Validate with Pydantic schema\n",
    "    result: AbstractAnalysis = AbstractAnalysis(**data)\n",
    "    \n",
    "    print(f\"‚úì Identified {len(result.key_themes)} key themes\")\n",
    "    print(f\"‚úì Selected {len(result.top_papers_for_deep_analysis)} papers for deep analysis\")\n",
    "    \n",
    "    return json.dumps(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 4: Download and Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client.models import operations, shared\n",
    "\n",
    "\n",
    "@agent.tool\n",
    "async def download_and_process_pdf(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    arxiv_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download and extract text from an arXiv paper PDF using Unstructured.io.\n",
    "    \n",
    "    Args:\n",
    "        arxiv_id: The arXiv ID (e.g., \"2301.12345\")\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text excerpt\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Processing PDF: {arxiv_id}\")\n",
    "    \n",
    "    # Check cache first\n",
    "    if arxiv_id in ctx.deps.papers_cache and \"fulltext\" in ctx.deps.papers_cache[arxiv_id]:\n",
    "        print(f\"‚úì Using cached fulltext\")\n",
    "        return ctx.deps.papers_cache[arxiv_id][\"fulltext\"]\n",
    "    \n",
    "    try:\n",
    "        # Download PDF from arXiv\n",
    "        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        response = requests.get(pdf_url, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Process with Unstructured.io Cloud API\n",
    "        req = operations.PartitionRequest(\n",
    "            partition_parameters=shared.PartitionParameters(\n",
    "                files=shared.Files(\n",
    "                    content=response.content,\n",
    "                    file_name=f\"{arxiv_id}.pdf\"\n",
    "                ),\n",
    "                strategy=shared.Strategy.HI_RES,\n",
    "                pdf_infer_table_structure=True,\n",
    "                skip_infer_table_types=[\"image\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        resp = ctx.deps.unstructured_client.general.partition(request=req)\n",
    "        \n",
    "        # Extract text from elements (elements are dicts, not objects)\n",
    "        text_parts: List[str] = []\n",
    "        for element in resp.elements:\n",
    "            # Handle both dict and object formats\n",
    "            if isinstance(element, dict):\n",
    "                text = element.get(\"text\", \"\")\n",
    "            elif hasattr(element, \"text\"):\n",
    "                text = element.text\n",
    "            else:\n",
    "                text = \"\"\n",
    "            \n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        \n",
    "        fulltext = \"\\n\".join(text_parts)\n",
    "        \n",
    "        # Limit length to stay within context window\n",
    "        excerpt = fulltext[:ctx.deps.fulltext_excerpt_chars]\n",
    "        \n",
    "        # Cache for reuse\n",
    "        if arxiv_id in ctx.deps.papers_cache:\n",
    "            ctx.deps.papers_cache[arxiv_id][\"fulltext\"] = excerpt\n",
    "        \n",
    "        print(f\"‚úì Extracted {len(fulltext):,} chars (using {len(excerpt):,} char excerpt)\")\n",
    "        \n",
    "        return excerpt\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to process {arxiv_id}: {str(e)}\"\n",
    "        print(f\"‚ö†Ô∏è  {error_msg}\")\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 5: Deep Analyze Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "async def deep_analyze_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    topic: str,\n",
    "    arxiv_ids: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform deep analysis of papers using their full text.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic\n",
    "        arxiv_ids: List of arXiv IDs to analyze\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with deep analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Deep analyzing {len(arxiv_ids)} papers...\")\n",
    "    \n",
    "    analyses: List[PaperAnalysis] = []\n",
    "    \n",
    "    for arxiv_id in arxiv_ids:\n",
    "        # Get fulltext (from cache or download)\n",
    "        if arxiv_id in ctx.deps.papers_cache and \"fulltext\" in ctx.deps.papers_cache[arxiv_id]:\n",
    "            fulltext = ctx.deps.papers_cache[arxiv_id][\"fulltext\"]\n",
    "        else:\n",
    "            fulltext = await download_and_process_pdf(ctx, arxiv_id)\n",
    "        \n",
    "        if \"Failed to process\" in fulltext:\n",
    "            continue\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this paper in the context of research on: \"{topic}\"\n",
    "\n",
    "Paper ID: {arxiv_id}\n",
    "\n",
    "Full text excerpt:\n",
    "{fulltext[:8000]}\n",
    "\n",
    "Extract:\n",
    "1. Methods and architectures used\n",
    "2. Novel contributions\n",
    "3. Limitations (if mentioned)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"arxiv_id\": \"{arxiv_id}\",\n",
    "  \"methods\": \"description\",\n",
    "  \"contributions\": \"description\",\n",
    "  \"limitations\": \"description or null\"\n",
    "}}\"\"\"\n",
    "\n",
    "        response = await ctx.deps.cerebras_client.chat.completions.create(\n",
    "            model=\"gpt-oss-120b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=1.0,\n",
    "            max_completion_tokens=12000\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        data = json.loads(content)\n",
    "        \n",
    "        # Validate with Pydantic schema\n",
    "        analysis: PaperAnalysis = PaperAnalysis(**data)\n",
    "        analyses.append(analysis)\n",
    "        \n",
    "        print(f\"  ‚úì Analyzed {arxiv_id}\")\n",
    "    \n",
    "    result = {\n",
    "        \"papers\": [a.model_dump() for a in analyses],\n",
    "        \"count\": len(analyses)\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì Completed deep analysis of {len(analyses)} papers\")\n",
    "    \n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 6: Synthesize Research Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "async def synthesize_research_findings(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    topic: str,\n",
    "    deep_analysis_json: str,\n",
    "    queries_used: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Synthesize all research findings into a comprehensive report.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic\n",
    "        deep_analysis_json: JSON string from deep_analyze_papers\n",
    "        queries_used: List of search queries that were used\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with final research output\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Synthesizing research findings...\")\n",
    "    \n",
    "    deep_analysis = json.loads(deep_analysis_json)\n",
    "    \n",
    "    # Safely get papers list with fallback\n",
    "    papers_list = deep_analysis.get('papers', [])\n",
    "    papers_count = deep_analysis.get('count', len(papers_list))\n",
    "    \n",
    "    if not papers_list:\n",
    "        # If no papers key, the JSON might be a single paper analysis or different format\n",
    "        # Try to handle it gracefully\n",
    "        print(\"‚ö†Ô∏è No 'papers' key found in deep_analysis_json, attempting to parse as single paper\")\n",
    "        if 'arxiv_id' in deep_analysis:\n",
    "            # It's a single paper analysis\n",
    "            papers_list = [deep_analysis]\n",
    "            papers_count = 1\n",
    "        else:\n",
    "            # Return a minimal synthesis\n",
    "            return json.dumps({\n",
    "                \"research_landscape_summary\": \"Unable to synthesize - no paper analysis data available.\",\n",
    "                \"key_innovations\": [],\n",
    "                \"future_research_directions\": [],\n",
    "                \"papers_analyzed\": 0,\n",
    "                \"queries_used\": queries_used\n",
    "            })\n",
    "    \n",
    "    papers_text = \"\\n\\n\".join([\n",
    "        f\"Paper {i+1} ({p.get('arxiv_id', 'unknown')}):\\n\"\n",
    "        f\"Methods: {p.get('methods', 'Not specified')}\\n\"\n",
    "        f\"Contributions: {p.get('contributions', 'Not specified')}\\n\"\n",
    "        f\"Limitations: {p.get('limitations', 'Not specified')}\"\n",
    "        for i, p in enumerate(papers_list)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Synthesize research findings on: \"{topic}\"\n",
    "\n",
    "Deep analysis of {papers_count} papers:\n",
    "\n",
    "{papers_text}\n",
    "\n",
    "Create a comprehensive research summary with:\n",
    "1. Research landscape overview (2-3 paragraphs)\n",
    "2. Key innovations (3-5 items)\n",
    "3. Future research directions (3-5 items with rationale)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"research_landscape_summary\": \"overview text\",\n",
    "  \"key_innovations\": [\"innovation1\", \"innovation2\", ...],\n",
    "  \"future_research_directions\": [\n",
    "    {{\"direction\": \"direction1\", \"rationale\": \"why\"}},\n",
    "    ...\n",
    "  ]\n",
    "}}\"\"\"\n",
    "\n",
    "    response = await ctx.deps.cerebras_client.chat.completions.create(\n",
    "        model=\"gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=1.0,\n",
    "        max_completion_tokens=12000\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    data = json.loads(content)\n",
    "    \n",
    "    # Add metadata\n",
    "    data[\"papers_analyzed\"] = papers_count\n",
    "    data[\"queries_used\"] = queries_used\n",
    "    \n",
    "    # Validate with Pydantic schema\n",
    "    result: ResearchOutput = ResearchOutput(**data)\n",
    "    \n",
    "    print(f\"‚úì Synthesis complete!\")\n",
    "    \n",
    "    return json.dumps(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 7: Save Research Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@agent.tool\n",
    "def save_research_report(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    topic: str,\n",
    "    research_output_json: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save the research report to a file.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic\n",
    "        research_output_json: JSON string from synthesize_research_findings\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ Saving research report...\")\n",
    "    \n",
    "    output = json.loads(research_output_json)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"research_exports\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"research_analysis_{timestamp}.txt\"\n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Safely get values with defaults\n",
    "    papers_analyzed = output.get('papers_analyzed', 'N/A')\n",
    "    research_landscape_summary = output.get('research_landscape_summary', 'No summary available.')\n",
    "    key_innovations = output.get('key_innovations', [])\n",
    "    future_research_directions = output.get('future_research_directions', [])\n",
    "    queries_used = output.get('queries_used', [])\n",
    "    \n",
    "    # Format report\n",
    "    report = f\"\"\"\n",
    "{'=' * 80}\n",
    "ACADEMIC RESEARCH ANALYSIS\n",
    "{'=' * 80}\n",
    "\n",
    "Topic: {topic}\n",
    "Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Papers Analyzed: {papers_analyzed}\n",
    "\n",
    "{'=' * 80}\n",
    "RESEARCH LANDSCAPE\n",
    "{'=' * 80}\n",
    "\n",
    "{research_landscape_summary}\n",
    "\n",
    "{'=' * 80}\n",
    "KEY INNOVATIONS\n",
    "{'=' * 80}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, innovation in enumerate(key_innovations, 1):\n",
    "        report += f\"{i}. {innovation}\\n\"\n",
    "    \n",
    "    report += f\"\\n{'=' * 80}\\nFUTURE RESEARCH DIRECTIONS\\n{'=' * 80}\\n\\n\"\n",
    "    \n",
    "    for i, direction in enumerate(future_research_directions, 1):\n",
    "        if isinstance(direction, dict):\n",
    "            report += f\"{i}. {direction.get('direction', 'Unknown')}\\n\"\n",
    "            report += f\"   Rationale: {direction.get('rationale', 'Not specified')}\\n\\n\"\n",
    "        else:\n",
    "            report += f\"{i}. {direction}\\n\\n\"\n",
    "    \n",
    "    report += f\"{'=' * 80}\\nSEARCH QUERIES USED\\n{'=' * 80}\\n\\n\"\n",
    "    \n",
    "    for i, query in enumerate(queries_used, 1):\n",
    "        report += f\"{i}. {query}\\n\"\n",
    "    \n",
    "    report += f\"\\n{'=' * 80}\\n\"\n",
    "    \n",
    "    # Save\n",
    "    filepath.write_text(report)\n",
    "    \n",
    "    print(f\"‚úì Report saved: {filepath}\")\n",
    "    \n",
    "    return str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Conversational Interface\n",
    "\n",
    "This function handles the conversation with the agent, including extracting the response from PydanticAI's message structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_agent(research_question: str, deps: ResearchDeps) -> str:\n",
    "    \"\"\"\n",
    "    Have a conversation with the research agent.\n",
    "    \n",
    "    Args:\n",
    "        research_question: The research question or instruction\n",
    "        deps: Research dependencies\n",
    "    \n",
    "    Returns:\n",
    "        Agent's response text\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã Your request: {research_question}\\n\")\n",
    "    \n",
    "    result = await agent.run(research_question, deps=deps)\n",
    "    \n",
    "    # Extract response from PydanticAI result\n",
    "    # The result contains new_messages() with TextPart and ThinkingPart objects\n",
    "    new_msgs = result.new_messages()\n",
    "    if new_msgs:\n",
    "        last_msg = new_msgs[-1]\n",
    "        if hasattr(last_msg, 'parts'):\n",
    "            # Extract only TextPart content, skip ThinkingPart\n",
    "            text_parts: List[str] = []\n",
    "            for part in last_msg.parts:\n",
    "                if hasattr(part, 'content') and 'TextPart' in str(type(part)):\n",
    "                    text_parts.append(part.content)\n",
    "            response = ' '.join(text_parts) if text_parts else str(last_msg)\n",
    "        else:\n",
    "            response = str(last_msg)\n",
    "    else:\n",
    "        response = str(result)\n",
    "    \n",
    "    print(\"üí¨ AGENT RESPONSE\")\n",
    "    print(f\"\\n{response}\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Run the Agent!\n",
    "\n",
    "### Instantiate Our Previously Created Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.cerebras.ai/v1/tcp_warming \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Start year: 2023\n",
      "   Max papers for deep analysis: 1\n",
      "   Fulltext excerpt: 12,000 chars\n"
     ]
    }
   ],
   "source": [
    "deps = create_research_deps(\n",
    "    start_year=2023,\n",
    "    max_papers_for_deep_analysis=1\n",
    ")\n",
    "print(f\"   Start year: {deps.start_year}\")\n",
    "print(f\"   Max papers for deep analysis: {deps.max_papers_for_deep_analysis}\")\n",
    "print(f\"   Fulltext excerpt: {deps.fulltext_excerpt_chars:,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Full Research Workflow\n",
    "\n",
    "The agent will autonomously:\n",
    "1. Generate search queries\n",
    "2. Search arXiv\n",
    "3. Analyze abstracts\n",
    "4. Download and process PDFs\n",
    "5. Perform deep analysis\n",
    "6. Synthesize findings\n",
    "7. Save the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Your request: \n",
      "Please conduct a comprehensive literature review on \"vision-language models for multimodal reasoning\".\n",
      "\n",
      "Follow these steps:\n",
      "1. Generate 3 diverse arXiv search queries\n",
      "2. Search arXiv with those queries\n",
      "3. Analyze the abstracts to identify key themes\n",
      "4. Select the top 1 most relevant paper\n",
      "5. Download and analyze that paper in depth\n",
      "6. Synthesize the findings into a comprehensive report\n",
      "7. Save the report to a file\n",
      "\n",
      "Provide a summary of your findings at the end.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating 3 search queries for: vision-language models for multimodal reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 3 queries\n",
      "  1. vision-language model AND multimodal reasoning AND cross-modal attention\n",
      "  2. pretrained VL transformer AND logical inference OR visual question answering AND zero-shot reasoning\n",
      "  3. multimodal reasoning benchmark AND vision-language pretraining AND contrastive learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Searching arXiv with 3 queries...\n",
      "‚úì Found 29 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analyzing abstracts for: vision-language models for multimodal reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Identified 11 key themes\n",
      "‚úì Selected 1 papers for deep analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing PDF: 2512.21583v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.unstructuredapp.io/general/docs \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Extracted 27,142 chars (using 12,000 char excerpt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Deep analyzing 1 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Analyzed 2512.21583v1\n",
      "‚úì Completed deep analysis of 1 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Synthesizing research findings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Synthesis complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving research report...\n",
      "‚úì Report saved: research_exports/research_analysis_20260120_105221.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ AGENT RESPONSE\n",
      "\n",
      "**Summary of Findings ‚Äì Vision‚ÄëLanguage Models for Multimodal Reasoning**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Generated Search Queries\n",
      "1. **vision-language model AND multimodal reasoning AND cross-modal attention**  \n",
      "2. **pretrained VL transformer AND logical inference OR visual question answering AND zero-shot reasoning**  \n",
      "3. **multimodal reasoning benchmark AND vision-language pretraining AND contrastive learning**\n",
      "\n",
      "These queries cover architecture, reasoning capabilities, and evaluation benchmarks.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Papers Retrieved & Abstract Themes\n",
      "From the combined search results (29 papers, 2023‚Äë2025), recurring themes include:\n",
      "- **Cross‚Äëmodal explainability** and attribution in large VLMs.  \n",
      "- **Taxonomies / surveys** of multimodal reasoning models.  \n",
      "- **Cross‚Äëmodal distillation** for handling incomplete modalities.  \n",
      "- **Application to object detection, segmentation, and open‚Äëvocabulary tasks**.  \n",
      "- **Knowledge conflict detection** and mitigation in VLMs.  \n",
      "- **Evaluation on niche tasks** (sarcasm detection, jailbreak robustness, causal reasoning).  \n",
      "- **In‚Äëcontext learning & task‚Äëaware demonstration selection**.  \n",
      "- **Transfer of reasoning across modalities** (vision ‚Üí audio, etc.).  \n",
      "- **Benchmarking chain‚Äëof‚Äëthought and causal reasoning**.  \n",
      "- **Logical/semantic supervision for interpretable VQA**.  \n",
      "- **Domain‚Äëspecific multimodal reasoning (e.g., medical AI).**\n",
      "\n",
      "The analysis identified **one paper as most directly relevant** to our core topic:\n",
      "\n",
      "**arXiv:2512.21583v1 ‚Äì ‚ÄúA Medical Multimodal Diagnostic Framework Integrating Vision‚ÄëLanguage Models and Logic Tree Reasoning.‚Äù**  \n",
      "\n",
      "---\n",
      "\n",
      "### 3. Deep Dive ‚Äì Selected Paper (2512.21583v1)\n",
      "\n",
      "#### Methodology\n",
      "- **Base Model:** LLaVA (large vision‚Äëlanguage model).  \n",
      "- **Components:**  \n",
      "  1. **Input Encoder** ‚Äì processes clinical text and medical images (CT/MRI).  \n",
      "  2. **Vision‚ÄëLanguage Alignment Module** ‚Äì projects visual tokens into the LLM hidden space (early multimodal interaction).  \n",
      "  3. **Reasoning Controller** ‚Äì decomposes diagnostic queries into a sequence of explicit chain‚Äëof‚Äëthought steps.  \n",
      "  4. **Logic‚ÄëTree Generator** ‚Äì assembles premises into a formal syllogistic structure (premise‚Äëpremise‚Äëconclusion) and enforces logical consistency via explicit regularization.  \n",
      "- **Inference Scheme:** Generates multiple candidate reasoning rollouts, refined through **DAPO** (Dynamic Adaptive Policy Optimization) balancing diagnostic accuracy, logical consistency, and image‚Äëtext grounding.  \n",
      "- **Outputs:** Traceable logic trees that clinicians can inspect, reducing hallucinations and improving auditability.\n",
      "\n",
      "#### Contributions\n",
      "1. **Vision‚ÄëLanguage Alignment + Logic Regularization** ‚Äì novel hybrid that improves multimodal reasoning fidelity.  \n",
      "2. **Explicit Reasoning Controller** ‚Äì stepwise premise generation curtails inconsistent chain‚Äëof‚Äëthoughts.  \n",
      "3. **Logic‚ÄëTree Generator** ‚Äì produces interpretable, verifiable reasoning structures.  \n",
      "4. **Empirical Gains:** Significant accuracy improvements on medical multimodal benchmarks (e.g., +20.8‚ÄØ% on MedXpertQA) while maintaining competitive text‚Äëonly performance.\n",
      "\n",
      "#### Limitations\n",
      "- Training instability in early explicit reasoning attempts.  \n",
      "- Heavy reliance on strong vision‚Äëlanguage alignment; logic alone insufficient.  \n",
      "- Evaluations limited to benchmark datasets ‚Äì real‚Äëworld clinical validation pending.  \n",
      "- Full fine‚Äëtuning of vision encoder required, increasing computational demand.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Synthesized Research Landscape\n",
      "\n",
      "**Current State:**  \n",
      "Vision‚Äëlanguage models have progressed from joint embedding learners (CLIP, ALIGN) to sophisticated multimodal reasoners (Flamingo, BLIP‚Äë2). In high‚Äëstakes domains like healthcare, there is a clear shift toward **hybrid neural‚Äësymbolic systems** that embed logical structure within deep models to bolster trustworthiness.\n",
      "\n",
      "**Key Innovations Identified**\n",
      "- Early, tight **vision‚Äëlanguage alignment** for shared multimodal representations.  \n",
      "- **Explicit, stepwise reasoning controllers** (chain‚Äëof‚Äëthought) that steer generation.  \n",
      "- **Logic‚Äëtree generation** for formal, auditable inference.  \n",
      "- Dynamic optimization (DAPO) balancing multiple objectives during inference.  \n",
      "- Demonstrated **accuracy and interpretability** gains on multimodal medical tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Future Research Directions\n",
      "\n",
      "| Direction | Rationale |\n",
      "|-----------|-----------|\n",
      "| **Joint pre‚Äëtraining with logic‚Äëaware objectives** | Embedding logical consistency during pre‚Äëtraining could reduce dependence on costly fine‚Äëtuning and improve inherent multimodal fidelity. |\n",
      "| **Scalable multimodal reasoning under limited compute** | Adapter modules or low‚Äërank updates could retain performance while mitigating the need to fully fine‚Äëtune heavy vision encoders. |\n",
      "| **Prospective clinical validation** | Real‚Äëworld deployment will uncover workflow, safety, and regulatory challenges beyond benchmark performance. |\n",
      "| **Incorporation of medical knowledge bases (UMLS, SNOMED)** | Leveraging structured ontologies can enhance factual correctness of premises and support more complex reasoning chains. |\n",
      "| **Robustness to noisy or occluded medical images** | Developing methods that sustain logical consistency under degraded inputs is essential for practical clinical use. |\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Report File\n",
      "The full synthesized report has been saved to:\n",
      "\n",
      "**`research_exports/research_analysis_20260120_105221.txt`**\n",
      "\n",
      "Feel free to open the file for the detailed JSON‚Äëformatted analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "research_question = \"\"\"\n",
    "Please conduct a comprehensive literature review on \"vision-language models for multimodal reasoning\".\n",
    "\n",
    "Follow these steps:\n",
    "1. Generate 3 diverse arXiv search queries\n",
    "2. Search arXiv with those queries\n",
    "3. Analyze the abstracts to identify key themes\n",
    "4. Select the top 1 most relevant paper\n",
    "5. Download and analyze that paper in depth\n",
    "6. Synthesize the findings into a comprehensive report\n",
    "7. Save the report to a file\n",
    "\n",
    "Provide a summary of your findings at the end.\n",
    "\"\"\"\n",
    "\n",
    "response = await chat_with_agent(research_question, deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Quick Abstract-Only Analysis\n",
    "\n",
    "The agent adapts to simpler requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Your request: \n",
      "What are the key themes in recent papers about \"persuasive natural language generation\"?\n",
      "Just analyze abstracts, don't download full papers.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating 5 search queries for: persuasive natural language generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 5 queries\n",
      "  1. persuasive natural language generation AND reinforcement learning\n",
      "  2. argumentative text generation models AND controllable generation\n",
      "  3. rhetorical structure theory AND neural language generation AND persuasion\n",
      "  4. user modeling AND persuasive dialogue systems AND GPT-4\n",
      "  5. multi-objective optimization AND persuasive NLG AND evaluation metrics\n",
      "\n",
      "üìö Searching arXiv with 5 queries...\n",
      "‚úì Found 36 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analyzing abstracts for: persuasive natural language generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Identified 8 key themes\n",
      "‚úì Selected 1 papers for deep analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ AGENT RESPONSE\n",
      "\n",
      "**Key Themes in Recent Persuasive Natural‚ÄëLanguage Generation (NLG) Research  \n",
      "(derived from abstract‚Äëlevel analysis of the ~20 most recent arXiv papers that mention ‚Äúpersuasive‚Äù or ‚Äúargumentative‚Äù generation)**  \n",
      "\n",
      "| # | Theme | What the literature focuses on | Representative recent paper(s) |\n",
      "|---|-------|--------------------------------|--------------------------------|\n",
      "| 1 | **Controllable persuasive dialogue generation** | Designs that let a system steer the conversation toward a persuasive goal (e.g., influencing opinions, prompting actions). Control is usually achieved through prompt engineering, conditional token‚Äëlevel biasing, or reinforcement‚Äëlearning (RL) signals. | **arXiv:2307.00161v1** ‚Äì ‚ÄúMixed‚Äëinitiative Controllable Dialogue Generation for Persuasion‚Äù (introduces a prompt‚Äëbased, mixed‚Äëinitiative framework that can be directed toward specific persuasive intents). |\n",
      "| 2 | **Mixed‚Äëinitiative & user‚Äëadapted persuasion** | Systems that allow the user (or a simulated partner) to intervene during generation, updating the model‚Äôs plan on the fly. Papers explore latent personality or belief representations that are updated with each turn, enabling personalized persuasive strategies. | arXiv:2307.00161v1 (mixed‚Äëinitiative aspect) plus related work on *latent persona embeddings* (cited in the same abstract). |\n",
      "| 3 | **Reinforcement‚Äëlearning signals for persuasion** | RL is used to optimize non‚Äëdifferentiable persuasion metrics such as belief change, intention to act, or persuasive effectiveness measured by human judges. Reward models are often trained on annotated persuasion datasets. | Abstracts of several 2023‚Äë24 papers (e.g., ‚ÄúRL‚Äëbased Persuasion Optimization‚Äù) mention leveraging RL to improve persuasive outcomes. |\n",
      "| 4 | **Prompt‚Äëbased generation with large language models (LLMs)** | Instead of fine‚Äëtuning, researchers craft task‚Äëspecific prompts (or prompt‚Äëtemplates) that coax LLMs (GPT‚Äë3/4, LLaMA, etc.) to generate persuasive arguments. Prompt engineering is coupled with few‚Äëshot examples or chain‚Äëof‚Äëthought reasoning to improve persuasiveness. | arXiv:2307.00161v1 (uses prompting rather than fine‚Äëtuning) and a 2024 survey on *prompt‚Äëcontrolled generation* (identified in the abstract search). |\n",
      "| 5 | **Counter‚Äëspeech & persuasive counter‚Äëarguments** | A related strand studies how to generate arguments that *counter* misinformation or extremist content. The goal is persuasive rebuttal rather than propaganda. Techniques include style‚Äëtransfer and argument mining. | Abstracts for a 2023 paper on *counter‚Äëspeech generation* appear under ‚Äúpersuasive content detection‚Äù. |\n",
      "| 6 | **Modeling & detecting persuasive content** | Before generation, many works focus on automatically identifying persuasive language features (rhetorical moves, sentiment, framing). These models inform downstream generation or serve as evaluation components. | Several 2023‚Äë24 abstracts discuss *persuasive content detection* using BERT‚Äëstyle classifiers. |\n",
      "| 7 | **Evaluation of persuasive effectiveness** | New benchmarks and metrics go beyond BLEU/ROUGE, incorporating human judgments of belief change, intent shift, or downstream action. Some works propose automatic predictors of persuasion success (e.g., belief‚Äëchange classifiers) to serve as reward signals. | The evaluation section of arXiv:2307.00161v1 describes the *PersuasionForGood* dataset and both human and automatic metrics for persuasion. |\n",
      "| 8 | **Multi‚Äëobjective optimization in persuasive NLG** | Researchers treat persuasion as one of several objectives (fluency, factuality, ethical constraints). Techniques include Pareto‚Äëoptimal search, constrained decoding, or hierarchical planning. | Abstracts mentioning ‚Äúmulti‚Äëobjective optimization‚Äù (e.g., a 2024 survey on *controllable generation* that lists persuasion as a target objective). |\n",
      "\n",
      "### How the Themes Were Identified\n",
      "1. **Search queries** were crafted to capture the breadth of the field (reinforcement learning, controllable generation, rhetorical structure theory, user modeling, multi‚Äëobjective optimization).  \n",
      "2. **arXiv abstracts** (up to 20 recent papers) were parsed automatically. The algorithm extracted recurring keywords, methodological families, and evaluation practices.  \n",
      "3. **Clustering** of these keywords yielded the eight high‚Äëlevel themes above.  \n",
      "4. The most directly relevant paper‚Äî**arXiv:2307.00161v1**‚Äîexplicitly tackles controllable, mixed‚Äëinitiative persuasive dialogue generation and therefore serves as an anchor for the thematic summary.\n",
      "\n",
      "### Take‚Äëaway for Researchers\n",
      "* **Control mechanisms** (prompt engineering, RL, conditional decoding) dominate current work; integrating them with user modeling is a promising next step.  \n",
      "* **Evaluation** is shifting toward human‚Äëin‚Äëthe‚Äëloop belief‚Äëchange metrics‚Äîfuture studies should adopt or extend the *PersuasionForGood* benchmark.  \n",
      "* **Ethical safeguards** (e.g., counter‚Äëspeech, detection of manipulative language) are increasingly coupled with generation, suggesting joint modeling pipelines as a fertile research direction.  \n",
      "\n",
      "These themes collectively map the state‚Äëof‚Äëthe‚Äëart in persuasive NLG as of 2023‚Äë2024 and provide a clear roadmap for deeper exploration, model building, and evaluation design.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_question = \"\"\"\n",
    "What are the key themes in recent papers about \"persuasive natural language generation\"?\n",
    "Just analyze abstracts, don't download full papers.\n",
    "\"\"\"\n",
    "\n",
    "response = await chat_with_agent(quick_question, deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Follow-up Questions\n",
    "\n",
    "The agent can answer follow-up questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Your request: What were the most innovative methods you found in those papers?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ AGENT RESPONSE\n",
      "\n",
      "I‚Äôm happy to dive into the most innovative methods described in the relevant papers, but I need a bit more context to make sure I‚Äôm looking at the right literature.\n",
      "\n",
      "Could you please tell me the specific research topic (or a short description of the area) you‚Äôd like me to review? Once I have that, I‚Äôll:\n",
      "\n",
      "1. Generate a set of diverse arXiv search queries for the topic.  \n",
      "2. Run those queries and collect the most pertinent papers.  \n",
      "3. Analyze abstracts to pinpoint the strongest candidates.  \n",
      "4. Pull the full texts, conduct a deep‚Äëdive analysis, and extract the cutting‚Äëedge methods.  \n",
      "5. Summarize the findings in a concise report.\n",
      "\n",
      "Just let me know the focus (e.g., ‚Äúgraph neural networks for molecular property prediction,‚Äù ‚Äúself‚Äësupervised learning for medical imaging,‚Äù etc.), and I‚Äôll get started!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followup = \"What were the most innovative methods you found in those papers?\"\n",
    "\n",
    "response = await chat_with_agent(followup, deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Inspect Results\n",
    "\n",
    "### View Cached Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers in cache: 36\n",
      "\n",
      "Cached papers:\n",
      "1. 2409.18827v1 ‚Äî ARLBench: Flexible and Efficient Benchmarking for Hyperparam...\n",
      "2. 2306.06371v1 ‚Äî A Comprehensive Review of State-of-The-Art Methods for Java ...\n",
      "3. 2312.04736v1 ‚Äî Is Feedback All You Need? Leveraging Natural Language Feedba...\n",
      "4. 2301.08028v4 ‚Äî A Tutorial on Meta-Reinforcement Learning...\n",
      "5. 2502.14437v1 ‚Äî Natural Language Generation...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Papers in cache: {len(deps.papers_cache)}\")\n",
    "print(\"\\nCached papers:\")\n",
    "for i, (arxiv_id, paper) in enumerate(list(deps.papers_cache.items())[:5], 1):\n",
    "    print(f\"{i}. {arxiv_id} ‚Äî {paper['title'][:60]}...\")\n",
    "    if 'fulltext' in paper:\n",
    "        print(f\"   ‚úì Full text cached ({len(paper['fulltext']):,} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Saved Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reports (4):\n",
      "  ‚Ä¢ research_analysis_20260120_105221.txt (4,876 bytes)\n",
      "  ‚Ä¢ research_analysis_20260118_132501.txt (10,000 bytes)\n",
      "  ‚Ä¢ research_analysis_20260118_085535.txt (11,756 bytes)\n",
      "  ‚Ä¢ research_analysis_20260117_212849.txt (7,760 bytes)\n"
     ]
    }
   ],
   "source": [
    "export_dir = Path(\"research_exports\")\n",
    "if export_dir.exists():\n",
    "    reports = sorted(export_dir.glob(\"*.txt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    print(f\"Saved reports ({len(reports)}):\")\n",
    "    for report in reports[:5]:\n",
    "        size = report.stat().st_size\n",
    "        print(f\"  ‚Ä¢ {report.name} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"No reports saved yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A **conversational academic research agent** with:\n",
    "\n",
    "- **tools** for a complete research workflow\n",
    "- **PydanticAI** for agent orchestration and tool management\n",
    "- **Cerebras** `gpt-oss-120b` for fast, high-quality reasoning\n",
    "- **Unstructured.io** for PDF text extraction\n",
    "- **Pydantic schemas** for type-safe structured outputs\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "1. **Cerebras Strict Mode**: Use `prepare_tools` hook to normalize all tools to `strict=False`\n",
    "2. **Dependency Injection**: Use `RunContext[ResearchDeps]` to share API clients and caches\n",
    "3. **Schema Validation**: Validate all LLM outputs with Pydantic models\n",
    "4. **Error Resilience**: Tools return error messages instead of raising exceptions\n",
    "5. **Caching**: Cache papers and full text to avoid redundant API calls\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Add semantic search with vector embeddings, rather than different API calls to arxiv's API\n",
    "- Add a citation graph analysis\n",
    "- Add multi-source search (PubMed, Semantic Scholar)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Cerebras Inference Docs](https://inference-docs.cerebras.ai)\n",
    "- [PydanticAI Docs](https://ai.pydantic.dev)\n",
    "- [Unstructured.io Docs](https://docs.unstructured.io)\n",
    "- [arXiv API](https://info.arxiv.org/help/api/basics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "Thank you team from Pydantic AI and Unstructured.io for incredibly helpful inputs during the creation of this cookbook. \n",
    "Also a shoutout to my colleagues Zhenwei Gao, Ryan Loney and Sarah Chieng for great feedback on initial versions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

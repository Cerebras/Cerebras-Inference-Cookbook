{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87d0ff5-b3c9-4c06-a324-70a9d2fb5d77",
   "metadata": {},
   "source": [
    " # Implementing Gist Memory: Summarizing and Searching Long Documents with a ReadAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666df92-9b07-43ba-9577-5b5c97ec8a6e",
   "metadata": {},
   "source": [
    "*Special thanks to Rohan Deshpande for the original implementation of this agent during his time at Cerebras!*\n",
    "\n",
    "Reading and reasoning over long documents like academic papers or legal texts presents a major challenge for AI due to the context window limitations of Large Language Models (LLMs). To solve this, we will build an agent that implements Gist Memory, a technique developed by Google DeepMind that mimics human reading patterns. Instead of ingesting a whole document at once, the agent intelligently breaks it into pages, creates high-level summaries (\"gists\") of each one, and then selectively re-reads only the most relevant sections to answer questions.\n",
    "\n",
    "The agent's workflow is entirely self-contained. It begins with a single ArXiv URL and processes it in memory through a structured sequence of steps. It first parses the document into clean text, then paginates it into semantic episodes. From there, it generates a concise gist for each page, creating a dual-memory structure: the full-text original pages and a parallel list of their corresponding summaries. This allows the agent to hold a compressed version of the entire document in memory.\n",
    "\n",
    "At the heart of this agent is the Gist Memory technique, which relies on two key LLM-driven stages: intelligent pagination to create coherent chunks and interactive lookup to retrieve relevant information on demand. This multi-step process requires dozens of sequential LLM calls, making fast inference essential for a responsive user experience. To handle document ingestion, we leverage a helper script that converts ArXiv papers into a clean HTML format using the ar5iv service. In this cookbook recipe, we'll walk you through how to build this Gist Agent step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06bf3a-4c57-49f7-89eb-5b2c8d1bdf9d",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "The agent's architecture can be understood as a sequence of four internal modules that work together to read, remember, and reason about a long document:\n",
    "\n",
    "* **Parser**: Fetches an ArXiv paper, converts it to HTML, and extracts a clean list of paragraphs for processing.\n",
    "* **Paginator**: Breaks the long list of paragraphs into semantically coherent \"pages\" by using an LLM to identify natural breakpoints in the text.\n",
    "* **Summarizer**: Reads each page and generates a concise \"gist\" to be stored in the agent's memory.\n",
    "* **Q\\&A Engine**: When asked a question, it first consults the list of gists to decide which pages are relevant, retrieves the full text for only those pages, and then generates an answer based on the enriched context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46d3c8-7fb1-492a-81c4-7ea0ac571723",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before getting started, please ensure that:\n",
    "\n",
    "* You have installed the Cerebras Inference SDK\n",
    "* You have a [Cerebras API key](https://cloud.cerebras.ai) and have saved it to an .env file as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b26991-9d75-4265-a540-eb2a376ff0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"CEREBRAS_API_KEY=your-api-key-here\" > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1e9d50-368b-4060-b98c-3733de7d214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cerebras-cloud-sdk in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (1.29.0)\n",
      "Requirement already satisfied: requests in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from cerebras-cloud-sdk) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras-cloud-sdk) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Applications/conda-new/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (2.27.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install cerebras-cloud-sdk requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988544a5-a869-49ef-b968-17821932b8a8",
   "metadata": {},
   "source": [
    "Next, import the necessary packages we will be using in this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfc59c2-699a-4a55-8df9-c440e23ae09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Union, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from cerebras.cloud.sdk import Cerebras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c839f7-0274-4a37-b16c-78347ff62ac6",
   "metadata": {},
   "source": [
    "And finally, load the environment variables and confirm that your API key is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645bf76e-9be7-4783-ad3c-9e5dcafdbe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key loaded successfully from .env file.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the API key is available\n",
    "if not os.getenv(\"CEREBRAS_API_KEY\"):\n",
    "    print(\"ðŸ›‘ Error: CEREBRAS_API_KEY not found.\")\n",
    "    print(\"Please ensure you have created a .env file with your API key.\")\n",
    "else:\n",
    "    print(\"âœ… API key loaded successfully from .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f4930-0ccb-495f-8761-ebdcb5fa1a82",
   "metadata": {},
   "source": [
    "## Parsing Arxiv Papers\n",
    "\n",
    "Before the agent can read a document, it needs clean, machine-readable text. Below we will define a number of functions that handle this by fetching an academic paper from ArXiv and converting it into a simple list of paragraphs. We will transform the ArXiv link into its corresponding ar5iv HTML version, which is much easier to process with standard tools.\n",
    "\n",
    "The parser's logic is built around a few key functions:\n",
    "\n",
    "* `get_ar5iv_link(url)`: This function takes a standard ArXiv URL for a PDF or abstract page and converts it into the equivalent ar5iv.labs.arxiv.org HTML link. It uses a regular expression to extract the paper's unique ID to build the new URL.\n",
    "\n",
    "* `get_html_page(url)`: To avoid re-downloading the same paper, this function fetches the HTML and saves it to a local html\\_cache directory. On subsequent runs, if the file exists in the cache, it's read directly from the disk.\n",
    "\n",
    "* `get_paragraphs_from_html(html)`: This function does the main work of text extraction. Using the BeautifulSoup library, it finds all paragraph elements in the HTML. It also includes a crucial preprocessing step for scientific content: it finds all mathematical formula tags (`<math>`) and replaces them with their readable LaTeX alttext, wrapped in \\$ symbols so the LLM can understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eeb4efa-77d8-4c38-bdf3-c3f2ab6db900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ar5iv_link(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Turns an arxiv link into a ar5iv link for HTML processing.\n",
    "\n",
    "    Args:\n",
    "        url (str): The original arxiv URL (e.g., https://arxiv.org/pdf/...).\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding ar5iv URL.\n",
    "    \"\"\"\n",
    "    if url.startswith(\"https://ar5iv.labs.arxiv.org/html/\"):\n",
    "        return url\n",
    "    \n",
    "    # Updated regex to handle different arxiv URL formats (e.g. /abs/, /pdf/)\n",
    "    match = re.search(r\"arxiv\\.org\\/(?:pdf|abs)\\/([\\w+.-]+)\", url)\n",
    "    if not match:\n",
    "        raise ValueError(f\"{url} is not a valid arxiv link!\")\n",
    "\n",
    "    paper_id = match.group(1)\n",
    "    # Remove .pdf if it exists\n",
    "    if paper_id.endswith('.pdf'):\n",
    "        paper_id = paper_id[:-4]\n",
    "        \n",
    "    return f\"https://ar5iv.labs.arxiv.org/html/{paper_id}\"\n",
    "\n",
    "\n",
    "def get_html_page(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches HTML content from a URL, using a local cache to avoid repeated requests.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The HTML content of the page.\n",
    "    \"\"\"\n",
    "    cache_dir = \"html_cache\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    # Create a simple cache key from the URL\n",
    "    cache_key = \"\".join(c for c in url if c.isalnum()) + \".html\"\n",
    "    file_path = os.path.join(cache_dir, cache_key)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Cache hit for {url}. Reading from {file_path}\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print(f\"Cache miss for {url}. Fetching from web...\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "            \n",
    "            html_content = response.text\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_content)\n",
    "            return html_content\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_title_from_html(html: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the document title from the ar5iv HTML.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The extracted title, or None if not found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    element = soup.find(class_=\"ltx_title_document\")\n",
    "    if element:\n",
    "        # Join fragments and strip whitespace for a clean title\n",
    "        title = \" \".join(element.get_text(strip=True).split())\n",
    "        return title\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_paragraphs_from_html(html: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extracts paragraphs from the ar5iv HTML.\n",
    "\n",
    "    Returns both a clean text version for the LLM and the original HTML\n",
    "    for potential rendering (though rendering is removed in this version).\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: A tuple containing:\n",
    "            - A list of LLM-readable paragraphs (clean text).\n",
    "            - A list of the original HTML paragraphs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Start searching for paragraphs after the main title\n",
    "    title_element = soup.find(class_=\"ltx_title_document\")\n",
    "    \n",
    "    search_area = title_element if title_element else soup\n",
    "    elements = search_area.find_all_next(class_=\"ltx_p\")\n",
    "\n",
    "    if not elements: # Fallback if no paragraphs are found after the title\n",
    "        elements = soup.find_all(class_=\"ltx_p\")\n",
    "\n",
    "    original_html = [str(e) for e in elements]\n",
    "    llm_readable = []\n",
    "\n",
    "    for e in elements:\n",
    "        # Create a copy to avoid modifying the original soup object\n",
    "        e_copy = BeautifulSoup(str(e), 'html.parser')\n",
    "        \n",
    "        # Replace <math> tags with their 'alttext' for better LLM consumption\n",
    "        for math_tag in e_copy.find_all('math'):\n",
    "            alttext = math_tag.get(\"alttext\")\n",
    "            if alttext:\n",
    "                # Wrap in $ to signify it's a formula\n",
    "                math_tag.replace_with(f\"${alttext.strip()}$\")\n",
    "        \n",
    "        text = e_copy.get_text(separator=' ', strip=True)\n",
    "        if text: # Only add non-empty paragraphs\n",
    "            llm_readable.append(text)\n",
    "\n",
    "    return llm_readable, original_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89cbed-38f4-4dc2-8198-0f89cbbdb09e",
   "metadata": {},
   "source": [
    "## Building the GistAgent Class\n",
    "\n",
    "Now that we have all of our helper functions available to handle papers from Arxiv, we can proceed to building the core logic of ReadAgent. The code can be found below as a part of the `GistAgent` class. \n",
    "\n",
    "### Intelligent Pagination\n",
    "\n",
    "Once the document is parsed into paragraphs, the next step is to group them into coherent \"pages.\" Instead of creating naive, fixed-size chunks that might awkwardly split a sentence or idea, the agent uses an LLM to find logical breakpoints. This process, called Episode Pagination, is handled by the \\_get\\_next\\_page\\_break method.\n",
    "\n",
    "The pagination logic works as follows:\n",
    "\n",
    "* Accumulate and Mark Text: The agent gathers paragraphs into a chunk of about 600 words. After a certain threshold, it begins inserting numbered labels (e.g., `<57>`) between paragraphs . These labels correspond to the paragraph's index in the full document.\n",
    "\n",
    "* Ask the LLM for a Breakpoint: This chunk, now containing embedded labels, is sent to the LLM. The prompt asks the model to choose the label that marks a \"natural\" place to break reading, such as a narrative transition or the end of an argument .\n",
    "\n",
    "* Set the Page Boundary: The agent parses the LLM's response to extract the chosen label (e.g., `<57>`). If the label is valid, that paragraph index is used as the end of the current page. If the LLM fails to provide a valid break, the agent defaults to ending the page at the end of the accumulated chunk.\n",
    "\n",
    "This iterative process is repeated until the entire document is divided. The result is a list of pages stored in self.pages, where each page is a semantically coherent section of the paper, ready for the next step.\n",
    "\n",
    "\n",
    "### Summarization (Memory Gisting)\n",
    "\n",
    "With the document now organized into coherent pages, the next step is to create a concise summary for each one. These summaries, or \"gists,\" form the agent's Gist Memoryâ€”a condensed, high-level version of the entire document that can be quickly scanned later. This process is handled by the \\_create\\_summary method.\n",
    "\n",
    "The summarization logic for each page is straightforward:\n",
    "\n",
    "* The agent takes the full text of a page and inserts it into a simple, direct prompt defined by `PROMPT_SHORTEN_TEMPLATE`. This prompt instructs the LLM to \"Please shorten the following passage. Just give me a shortened version. DO NOT explain your reason\" .\n",
    "* The LLM's response is then cleaned by a helper function, `_post_process_summary`, which strips away any conversational filler (e.g., \"Here is the shortened version:\") to ensure the gist is clean.\n",
    "\n",
    "This process is repeated for every page in the document. At the end of this stage, the agent holds two parallel data structures in its memory: `self.pages` (a list of the original, full-text pages) and `self.shortened_pages` (a list of the corresponding gists). This dual-memory system is the core of the Gist Memory technique and is essential for the final question-answering stage.\n",
    "\n",
    "### The Q\\&A Engine with Interactive Lookup\n",
    "\n",
    "This final stage is where the agent uses its Gist Memory to answer questions about the document. Instead of overwhelming the LLM with the full text, the answer method orchestrates a two-step \"lookup then answer\" process that allows the agent to focus only on the most relevant information.\n",
    "\n",
    "The Q\\&A logic works as follows:\n",
    "\n",
    "**The Lookup Stage**: The agent first needs to decide which parts of the document to re-read.\n",
    "\n",
    "* It compiles all the gists into a single \"memory\" text, where each gist is labeled with its page number.\n",
    "* Using the `PROMPT_LOOKUP_TEMPLATE`, it presents this gist memory and the user's question to the LLM. The prompt specifically instructs the model not to answer the question yet, but to instead identify which pages it needs to read in full to find the answer.\n",
    "* The agent parses the page numbers from the model's response (e.g., \"I want to look up Page \\[2, 5]...\").\n",
    "\n",
    "**The Answering Stage**: With the relevant page numbers identified, the agent constructs a final, enriched context to generate the answer.\n",
    "\n",
    "* It starts with the list of all page gists.\n",
    "* It then iterates through the page numbers chosen during the lookup stage and replaces their gists with the original, full-text versions from self.pages. The result is a hybrid context containing high-detail excerpts where needed and low-detail summaries everywhere else.\n",
    "* Finally, using the `PROMPT_FREE_ANSWER_TEMPLATE`, the agent sends this hybrid context and the user's question to the LLM to generate the final, fully-informed answer.\n",
    "\n",
    "By following this process, the agent intelligently consults its memory to retrieve only the most pertinent details on demand, allowing it to provide accurate answers based on documents that are far too long to fit in a single context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56010bf-5025-49a9-a5d0-cb9351e01dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Gist Memory Prompts ---\n",
    "\n",
    "PROMPT_PAGINATION_TEMPLATE = \"\"\"\n",
    "You are given a passage that is taken from a larger text (article, book, ...) and some numbered labels between the paragraphs in the passage.\n",
    "Numbered label are in angeled brackets. For example, if the label number is 19, it shows as <19> in text.\n",
    "Please choose one label that it is natural to break reading.\n",
    "Such point can be scene transition, end of a dialogue, end of an argument, narrative transition, etc.\n",
    "Please answer the break point label and explain.\n",
    "For example, if <57> is a good point to break, answer with \\\"Break point: <57>\\n Because ...\\\"\n",
    "\n",
    "Passage:\n",
    "\n",
    "{0}\n",
    "{1}\n",
    "{2}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_SHORTEN_TEMPLATE = \"\"\"\n",
    "Please shorten the following passage.\n",
    "Just give me a shortened version. DO NOT explain your reason.\n",
    "\n",
    "Passage:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_LOOKUP_TEMPLATE = \"\"\"\n",
    "The following text is what you remembered from reading an article and a multiple choice question related to it.\n",
    "You may read 1 to 6 page(s) of the article again to refresh your memory to prepare yourself for the question.\n",
    "Please respond with which page(s) you would like to read.\n",
    "For example, if your only need to read Page 8, respond with \\\"I want to look up Page [8] to ...\\\";\n",
    "if your would like to read Page 7 and 12, respond with \\\"I want to look up Page [7, 12] to ...\\\";\n",
    "if your would like to read Page 2, 3, 7, 15 and 18, respond with \\\"I want to look up Page [2, 3, 7, 15, 18] to ...\\\".\n",
    "if your would like to read Page 3, 4, 5, 12, 13 and 16, respond with \\\"I want to look up Page [3, 3, 4, 12, 13, 16] to ...\\\".\n",
    "DO NOT select more pages if you don't need to.\n",
    "DO NOT answer the question yet.\n",
    "\n",
    "Text:\n",
    "{}\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Take a deep breath and tell me: Which page(s) would you like to read again?\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_FREE_ANSWER_TEMPLATE = \"\"\"\n",
    "Read the following article and then answer the question.\n",
    "\n",
    "Article:\n",
    "{}\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e299dd2f-0dcb-4819-92fd-41ef08e6c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClientContainer:\n",
    "    \"\"\"A simple data class to hold the LLM client and model name.\"\"\"\n",
    "    client: Cerebras\n",
    "    model: str\n",
    "\n",
    "class GistAgent:\n",
    "    \"\"\"\n",
    "    An AI agent that uses Gist Memory to process and answer questions about long documents.\n",
    "    This implementation is stripped of UI components and focuses on the core logic for Cerebras models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"llama-3.3-70b\"):\n",
    "        \"\"\"\n",
    "        Initializes the GistAgent.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): Your Cerebras API key.\n",
    "            model (str): The model ID to use for inference.\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"Cerebras API key is required.\")\n",
    "        \n",
    "        print(f\"Initializing agent with model: {model}\")\n",
    "        self.client_container = self._get_client(model, api_key)\n",
    "        self.title = \"\"\n",
    "        self.pages: List[List[str]] = []\n",
    "        self.shortened_pages: List[str] = []\n",
    "\n",
    "    def _get_client(self, model: str, api_key: str) -> ClientContainer:\n",
    "        \"\"\"\n",
    "        Initializes and returns the Cerebras client.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client = Cerebras(api_key=api_key)\n",
    "            print(\"Cerebras client initialized successfully.\")\n",
    "            return ClientContainer(client, model)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Cerebras client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _run_llm(self, messages: List[Dict[str, str]], stream: bool = False) -> Union[str, None]:\n",
    "        \"\"\"\n",
    "        Runs an LLM inference call.\n",
    "\n",
    "        Args:\n",
    "            messages (List[Dict[str, str]]): The message history for the prompt.\n",
    "            stream (bool): Whether to use streaming.\n",
    "\n",
    "        Returns:\n",
    "            The LLM's response content as a string, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client_container.client.chat.completions.create(\n",
    "                model=self.client_container.model,\n",
    "                messages=messages,\n",
    "                stream=stream,\n",
    "            )\n",
    "            \n",
    "            if not stream:\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                full_response = \"\"\n",
    "                final_chunk = None\n",
    "                for chunk in response:\n",
    "                    if chunk.choices[0].delta.content:\n",
    "                        full_response += chunk.choices[0].delta.content\n",
    "                    if chunk.choices[0].finish_reason is not None:\n",
    "                        final_chunk = chunk\n",
    "                return full_response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during LLM call: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_next_page_break(self, paragraphs: List[str], start_paragraph: int) -> int:\n",
    "        \"\"\"\n",
    "        Determines the next natural break point in the document.\n",
    "\n",
    "        Args:\n",
    "            paragraphs (List[str]): The list of all paragraphs in the document.\n",
    "            start_paragraph (int): The index of the paragraph to start from.\n",
    "\n",
    "        Returns:\n",
    "            The index of the paragraph that marks the end of the new page.\n",
    "        \"\"\"\n",
    "        word_limit = 600\n",
    "        start_threshold = 280\n",
    "        \n",
    "        i = start_paragraph\n",
    "        preceding = \"\" if i == 0 else \"...\\n\" + '\\n'.join(self.pages[-1])\n",
    "        \n",
    "        passage = [paragraphs[i]]\n",
    "        wcount = len(paragraphs[i].split())\n",
    "        j = i + 1\n",
    "        \n",
    "        while wcount < word_limit and j < len(paragraphs):\n",
    "            wcount += len(paragraphs[j].split())\n",
    "            if wcount >= start_threshold:\n",
    "                passage.append(f\"<{j}>\")\n",
    "            passage.append(paragraphs[j])\n",
    "            j += 1\n",
    "        passage.append(f\"<{j}>\")\n",
    "        \n",
    "        end_tag = \"\" if j == len(paragraphs) else paragraphs[j] + \"\\n...\"\n",
    "\n",
    "        if wcount < 350:\n",
    "            return len(paragraphs)\n",
    "\n",
    "        prompt = PROMPT_PAGINATION_TEMPLATE.format(preceding, '\\n'.join(passage), end_tag)\n",
    "        response = self._run_llm([{\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        if response:\n",
    "            pause_point = self._parse_pause_point(response)\n",
    "            if pause_point and (pause_point > i and pause_point <= j):\n",
    "                return pause_point\n",
    "        \n",
    "        # Fallback to the max paragraph count in this chunk\n",
    "        return j\n",
    "\n",
    "    def _parse_pause_point(self, text: str) -> Union[int, None]:\n",
    "        \"\"\"Parses the pause point label (e.g., '<57>') from the LLM response.\"\"\"\n",
    "        match = re.search(r\"<(\\d+)>\", text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    def _post_process_summary(self, text: str) -> str:\n",
    "        \"\"\"Removes conversational prefixes from summaries.\"\"\"\n",
    "        match = re.match(r\"(here[a-z ]+ shortened.*?:)\", text.lower())\n",
    "        if match:\n",
    "            text = text[len(match.group(1)) :].strip()\n",
    "        return text\n",
    "\n",
    "    def _create_summary(self, page: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Creates a summary (gist) for a given page of text.\n",
    "        \"\"\"\n",
    "        prompt = PROMPT_SHORTEN_TEMPLATE.format('\\n'.join(page))\n",
    "        response = self._run_llm([{\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        if response:\n",
    "            shortened_text = response.strip()\n",
    "            return self._post_process_summary(shortened_text)\n",
    "        return \"Failed to generate summary.\"\n",
    "\n",
    "    def process_document(self, url: str):\n",
    "        \"\"\"\n",
    "        Processes an entire document from a URL, paginating and summarizing it.\n",
    "\n",
    "        Args:\n",
    "            url (str): The ArXiv URL of the paper to process.\n",
    "        \"\"\"\n",
    "        print(f\"Processing document from: {url}\")\n",
    "        ar5iv_url = get_ar5iv_link(url)\n",
    "        html_page = get_html_page(ar5iv_url)\n",
    "        \n",
    "        self.title = get_title_from_html(html_page)\n",
    "        if not self.title:\n",
    "            print(f\"Error: Could not parse title from {ar5iv_url}. The paper might not be supported by ar5iv.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*20)\n",
    "        print(f\"Title: {self.title}\")\n",
    "        print(\"=\"*20 + \"\\n\")\n",
    "        \n",
    "        paragraphs, _ = get_paragraphs_from_html(html_page)\n",
    "        \n",
    "        pause_point = 0\n",
    "        total_paragraphs = len(paragraphs)\n",
    "\n",
    "        while pause_point < total_paragraphs:\n",
    "            page_num = len(self.pages) + 1\n",
    "            print(f\"Processing Page {page_num}...\")\n",
    "            \n",
    "            old_pause_point = pause_point\n",
    "            new_pause_point = self._get_next_page_break(paragraphs, old_pause_point)\n",
    "            \n",
    "            current_page = paragraphs[old_pause_point:new_pause_point]\n",
    "            self.pages.append(current_page)\n",
    "            \n",
    "            summary = self._create_summary(current_page)\n",
    "            self.shortened_pages.append(summary)\n",
    "            \n",
    "            pause_point = new_pause_point\n",
    "            print(f\"Completed Page {page_num}. Progress: {pause_point}/{total_paragraphs} paragraphs processed.\")\n",
    "\n",
    "        print(\"\\nDocument processing complete.\\n\")\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Answers a question based on the processed document's gist memory.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user's question.\n",
    "        \"\"\"\n",
    "        if not self.pages:\n",
    "            print(\"Error: No document has been processed. Please call `process_document` first.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*20)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\"*20 + \"\\n\")\n",
    "\n",
    "        shortened_pages_pidx = [f\"\\nPage {i}:\\n{gist}\" for i, gist in enumerate(self.shortened_pages)]\n",
    "        shortened_article = '\\n'.join(shortened_pages_pidx)\n",
    "\n",
    "        # Step 1: Ask the model which pages to look up\n",
    "        prompt_lookup = PROMPT_LOOKUP_TEMPLATE.format(shortened_article, question)\n",
    "        print(\"Asking model for page lookup rationale...\")\n",
    "        intermediate_response = self._run_llm([{\"role\": \"user\", \"content\": prompt_lookup}])\n",
    "\n",
    "        if not intermediate_response:\n",
    "            print(\"Failed to get lookup response from LLM.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n--- Model's Lookup Rationale ---\")\n",
    "        print(intermediate_response.strip())\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "        page_ids = []\n",
    "        try:\n",
    "            match = re.search(r'\\[([\\d,\\s]+)\\]', intermediate_response)\n",
    "            if match:\n",
    "                page_ids_str = match.group(1).split(',')\n",
    "                for p in page_ids_str:\n",
    "                    if p.strip().isnumeric():\n",
    "                        page_id = int(p.strip())\n",
    "                        if 0 <= page_id < len(self.pages):\n",
    "                            page_ids.append(page_id)\n",
    "                        else:\n",
    "                            print(f\"  - (Model requested invalid page index: {page_id})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse page IDs from response: {e}\")\n",
    "        \n",
    "        chosen_pages = sorted(list(set(page_ids))) if page_ids else 'None'\n",
    "        print(f\"Model chose to re-read page(s): {chosen_pages}\\n\")\n",
    "\n",
    "        # Step 2: Construct the final context with expanded pages\n",
    "        expanded_shortened_pages = self.shortened_pages[:]\n",
    "        if page_ids:\n",
    "            for page_id in page_ids:\n",
    "                expanded_shortened_pages[page_id] = '\\n'.join(self.pages[page_id])\n",
    "\n",
    "        expanded_article = '\\n'.join(expanded_shortened_pages)\n",
    "\n",
    "        # Step 3: Ask the final question\n",
    "        prompt_answer = PROMPT_FREE_ANSWER_TEMPLATE.format(expanded_article, question)\n",
    "        \n",
    "        print(\"Generating final answer...\")\n",
    "        final_answer = self._run_llm([{\"role\": \"user\", \"content\": prompt_answer}])\n",
    "\n",
    "        if final_answer:\n",
    "            print(\"\\n--- Final Answer ---\")\n",
    "            print(final_answer.strip())\n",
    "            print(\"--------------------\\n\")\n",
    "        else:\n",
    "            print(\"Failed to generate a final answer.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ee200-c76e-490d-ba44-f33f9bf70727",
   "metadata": {},
   "source": [
    "## Putting It All Together: Running the Agent\n",
    "\n",
    "This final code block brings the ReadAgent to life. The script will prompt you for an ArXiv paper URL. If you press enter without providing one, it will automatically use the default URL for the Gist Memory paper. The agent then executes its primary process_document method, which fetches the paper, intelligently breaks it into pages, and creates a concise \"gist\" summary for each one.\n",
    "\n",
    "After the document has been fully processed, the agent enters an interactive Q&A loop. You can now ask specific questions about the paper, and the agent will use its gist memory to find the relevant context and generate a detailed answer. To end your session, simply type exit and press Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789d48c-cff8-445a-88a8-68fb121c788c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gist Memory Agent ---\n",
      "Initializing agent with model: llama-3.3-70b\n",
      "Cerebras client initialized successfully.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an ArXiv paper URL (e.g., https://arxiv.org/pdf/1706.03762):  https://arxiv.org/pdf/1706.03762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document from: https://arxiv.org/pdf/1706.03762\n",
      "Cache hit for https://ar5iv.labs.arxiv.org/html/1706.03762. Reading from html_cache/httpsar5ivlabsarxivorghtml170603762.html\n",
      "\n",
      "====================\n",
      "Title: Attention Is All You Need\n",
      "====================\n",
      "\n",
      "Processing Page 1...\n",
      "Completed Page 1. Progress: 4/66 paragraphs processed.\n",
      "Processing Page 2...\n",
      "Completed Page 2. Progress: 9/66 paragraphs processed.\n",
      "Processing Page 3...\n",
      "Completed Page 3. Progress: 13/66 paragraphs processed.\n",
      "Processing Page 4...\n",
      "Completed Page 4. Progress: 19/66 paragraphs processed.\n",
      "Processing Page 5...\n",
      "Completed Page 5. Progress: 28/66 paragraphs processed.\n",
      "Processing Page 6...\n",
      "Completed Page 6. Progress: 35/66 paragraphs processed.\n",
      "Processing Page 7...\n",
      "Completed Page 7. Progress: 41/66 paragraphs processed.\n",
      "Processing Page 8...\n",
      "Completed Page 8. Progress: 47/66 paragraphs processed.\n",
      "Processing Page 9...\n",
      "Completed Page 9. Progress: 57/66 paragraphs processed.\n",
      "Processing Page 10...\n",
      "Completed Page 10. Progress: 62/66 paragraphs processed.\n",
      "Processing Page 11...\n",
      "Completed Page 11. Progress: 66/66 paragraphs processed.\n",
      "\n",
      "Document processing complete.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question about the paper (or type 'exit' to quit):  Explain how self attention works?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Question: Explain how self attention works?\n",
      "====================\n",
      "\n",
      "Asking model for page lookup rationale...\n",
      "\n",
      "--- Model's Lookup Rationale ---\n",
      "I want to look up Page [2, 3, 4] to refresh my understanding of how self-attention works in the Transformer model, particularly the mechanics of the attention function, multi-head attention, and its application in the encoder and decoder.\n",
      "--------------------------------\n",
      "\n",
      "Model chose to re-read page(s): [2, 3, 4]\n",
      "\n",
      "Generating final answer...\n",
      "\n",
      "--- Final Answer ---\n",
      "Self-attention, also known as intra-attention, is a mechanism used in the Transformer model to allow the model to attend to different positions of the input sequence simultaneously and weigh their importance. Here's a step-by-step explanation of how self-attention works:\n",
      "\n",
      "1. **Input**: The input to the self-attention mechanism is a sequence of vectors, typically the output of the previous layer in the encoder or decoder. These vectors are called \"keys\", \"values\", and \"queries\".\n",
      "2. **Compute attention scores**: The self-attention mechanism computes attention scores by taking the dot product of the query vector with each key vector, and then applying a scaling factor and a softmax function. The attention scores represent the importance of each key vector with respect to the query vector.\n",
      "3. **Compute weighted sum**: The self-attention mechanism computes a weighted sum of the value vectors, where the weights are the attention scores computed in the previous step. This produces a new vector that represents the input sequence with the importance of each position weighted by the attention scores.\n",
      "4. **Multi-head attention**: The Transformer model uses a technique called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This is done by applying multiple attention mechanisms in parallel, each with a different set of learned linear projections.\n",
      "5. **Output**: The output of the self-attention mechanism is the weighted sum of the value vectors, which represents the input sequence with the importance of each position weighted by the attention scores.\n",
      "\n",
      "The self-attention mechanism can be mathematically represented as follows:\n",
      "\n",
      "`Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V`\n",
      "\n",
      "where:\n",
      "\n",
      "* `Q` is the query vector\n",
      "* `K` is the key vector\n",
      "* `V` is the value vector\n",
      "* `d_k` is the dimensionality of the key vector\n",
      "* `softmax` is the softmax function\n",
      "* `*` denotes the dot product\n",
      "\n",
      "The self-attention mechanism has several benefits, including:\n",
      "\n",
      "* **Parallelization**: Self-attention can be parallelized more easily than recurrent neural networks, making it faster to train and more efficient.\n",
      "* **Interpretability**: Self-attention can provide more interpretable results than recurrent neural networks, as the attention scores can be used to visualize the importance of each input position.\n",
      "* **Flexibility**: Self-attention can be used with different types of input data, including sequences, images, and graphs.\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"--- Gist Memory Agent ---\")\n",
    "        \n",
    "        CEREBRAS_API_KEY = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "\n",
    "        if not CEREBRAS_API_KEY:\n",
    "            print(\"\\nError: CEREBRAS_API_KEY environment variable not set.\")\n",
    "            print(\"Please set the environment variable before running the script.\")\n",
    "            print(\"Example: export CEREBRAS_API_KEY='your_key_here'\")\n",
    "        else:\n",
    "            # --- Initialization ---\n",
    "            agent = GistAgent(api_key=CEREBRAS_API_KEY)\n",
    "            \n",
    "            # --- Document Processing ---\n",
    "            arxiv_url = input(\"Enter an ArXiv paper URL (e.g., https://arxiv.org/pdf/1706.03762): \")\n",
    "            if not arxiv_url:\n",
    "                 arxiv_url = \"https://arxiv.org/pdf/2402.09727\" # Gist Memory paper\n",
    "                 print(f\"No URL provided, using default: {arxiv_url}\")\n",
    "\n",
    "            agent.process_document(arxiv_url)\n",
    "\n",
    "            # --- Q&A Loop ---\n",
    "            if agent.pages:\n",
    "                while True:\n",
    "                    user_question = input(\"\\nAsk a question about the paper (or type 'exit' to quit): \")\n",
    "                    if user_question.lower() == 'exit':\n",
    "                        break\n",
    "                    if user_question:\n",
    "                        agent.answer(user_question)\n",
    "\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50931ec4-dc61-42e8-9fc2-d1f191d422c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we built a Gist Agent capable of reading and reasoning about academic papers far longer than a typical LLM context window can support. By mimicking a human readerâ€™s strategy of semantic pagination, summarization, and targeted lookup, the agent intelligently overcomes the context limitations of standard models. This project serves as a powerful example of how to solve complex problems by breaking them into smaller, manageable parts and using an LLM as a component in a larger workflow.\n",
    "\n",
    "The Gist Memory agent demonstrates a fundamental shift in designing AI systems. Instead of relying on a single, massive prompt, we created an algorithmic workflow where the LLM is called dozens of times sequentially to paginate, summarize, and retrieve information. The modelâ€™s output from one step, such as the list of gists, directly informs the input for subsequent steps, like the lookup stage. This iterative, memory-augmented architecture represents a more sophisticated and capable approach to building AI agents.\n",
    "\n",
    "This new class of agent architecture is only practical with access to high-speed, low-latency inference. Processing a single document can require over 20 LLM calls, with additional calls needed for each question asked. If each call took several seconds, the user would be left waiting for minutes, rendering the agent useless for interactive Q\\&A. Fast inference is therefore not just a performance enhancement; it is the core enabling technology that makes complex, multi-step agentic workflows like this one viable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
